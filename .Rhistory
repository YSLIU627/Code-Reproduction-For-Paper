if (alpha[i] >= g_sub_left & alpha[i] <= g_sub_right){
return (0)
}
else {
return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
# Return the closet distance from the interval
}
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
p.ada.gap <- function(alpha,lambda,B,A,y){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- lasso.gap(alpha, lambda, B, A, i, y)
#browser()
}
#browser()
psum <- sum(p)
for (i in 1:n){
p[i] <- p[i]/psum
}
return(p)
}
p.ada.uniform <- function(alpha, lambda, A, y, sigma,B){
n <- length(alpha)
p <- numeric(length = n)
second_term <- numeric(length = n)
eps <- 1e-5
m <- n
for (i in 1:n){
k <- abs(lasso.dualres(alpha, lambda, A, i,y, B))
if( k < eps){
p[i] <- 0
m <- m-1
}
second_term[i] <- k*norm(as.matrix(A[,i]),type ="F")
#browser()
}
second_term <- second_term/sum(second_term)
for(i in 1:n){
if(second_term[i]<eps){
second_term[i] <- 0
}
}
second_term <- second_term/sum(second_term)
p <- sigma/m + second_term*(1-sigma)
for(i in 1:n){
if(second_term[i] == 0){
p[i] <- 0
}
}
#browser()
return(p)
}
p.uniform <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- 1/n
}
return(p)
}
p.imp <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- norm(as.matrix(A[,i]),type = "F")
}
p = p/sum(p)
return(p)
}
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
#browser()
B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha[i] <- alpha[i] - step0*update/inverses
#alpha_ <- alpha - step0*update/inverses
#if (lasso.loss(A,alpha_,y) < lasso.loss(A,alpha,y) ){
#   alpha <- alpha_
#   iter <- iter+1
#}
#if (iter %%10 == 0){
#  print("w")
#  w <- lasso.w_func(alpha,A,y)
#  print(sum(w**2))}
#browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
#browser()
}
}
result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
#browser()
return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
CD_per_epoch <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
#browser()
B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = p)
update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha[i] <- alpha[i] - step0*update/inverses
#browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
#browser()
}
}
result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
#browser()
return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
A.mushrooms <- matrix(0,nrow = 8124, ncol = 112)
y.mushrooms <- numeric(length = 112)
a <- strsplit(mushrooms, split = " ")
for(i in 1:8124){
b <- a[[i]]
y.mushrooms[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
for(j in 1:len){
colind <- c[[j]][1]
val <- c[[j]][2]
colind <- as.numeric(colind)
val <- as.numeric(val)
A.mushrooms[i,colind] <- val
}
}
set.seed(100)
ind <- sample(1:20242, 8000)
feature <- sample(1:50000, 300)
feature <- as.character(sort(feature))
rcv1 <- rcv1[ind]
A.rcv1 <- matrix(0,nrow = 8000, ncol = 300)
colnames(A.rcv1) <- feature
y.rcv1 <- numeric(length = 8000)
a <- strsplit(rcv1, split = " ")
for(i in 1:8000){
b <- a[[i]]
y.rcv1[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
#browser()
for(j in 1:len){
colind <- c[[j]][1]
if(sum(colind==feature)==1){
val <- c[[j]][2]
val <- as.numeric(val)
A.rcv1[i,colind] <- val
}
}
}
knitr::opts_chunk$set(echo = TRUE)
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.loss <- function(A,alpha,y) {
# 返回loss函数的值
return (norm(as.matrix(A%*%alpha - y),type = "2"))
#+norm("alpha",type = "O"))
}
lasso.w_func <- function(alpha,A,y){
#返回w的函数值
return(2*(A %*% alpha - y))
}
lasso.subgrad <- function(alpha,lambda,A,dimension,y,...){
#返回次梯度
i <- dimension
n <- length(alpha)
#alpha0 <- as.matrix(alpha)
# First calculate the subgrad of |alpha|, sub1
if (alpha[i] > 0 ){
sub1 <- lambda#alpha[i]/norm(alpha0,type = "O")
}
else if(alpha[i] < 0){
sub1 <- -lambda#alpha[i]/norm(alpha0,type = "O")
}
else {
seed <- 0#runif(1,-1,1)
sub1 <- lambda*seed #* alpha[i]/norm(alpha0,type = "O")
}
# Then calculate the grad of ||A\alpha -y||^2
#term <- vector(length =length(A[,1]))
#for (s in 1:n){
#  term <- term + as.vector(alpha[s]*A[,s])
#}
#sub2 <- t(A[,i]) %*% term + t(term) %*% A[,i] - t(y) %*% A[,i] - t(A[,i])%*% y
term <- 2*(A%*%alpha-y)
term <- as.vector(term)
sub2 <- sum(A[,i]*term)
subgrad = sub1 + sub2 #rowSums(sub2)
#browser()
return(subgrad)
}
lasso.gap <- function(alpha,lambda,B,A,dimension,y){
# 计算对偶gap
# A is a matrix
i <- dimension
#browser()
w <- lasso.w_func(alpha,A,y)
gap <- B*positive(abs(sum(A[,i]* w))- lambda)+lambda*abs(alpha[i])+alpha[i]*(sum(A[,i]* w))
#browser()
return(gap)
}
#CD_each_iter(sample_p = p.uniform,A =diag(1,3,3) ,y = diag(1,3,3)%*%c(1,0,1),lambda = 0.05)
lasso.dualres <- function(alpha,lambda,A,dimension,y, B){
#计算对偶残差
# first calculate the subgrad of g_i^*
i <- dimension
flag <-  0
eps <- 1e-5
w <- lasso.w_func(alpha, A, y)
input <- -t(A[,i])%*%w
#browser()
# 这里的g用的是文中修改后的g拔，下面计算次梯度
if (input <= -lambda - eps){
g_sub <- -B
}
else if(input >= lambda + eps){
g_sub <- B
}
else if(input > -lambda + eps & input < lambda + eps){
g_sub <- 0
}
else if(input < 0){
g_sub_right <- 0
g_sub_left  <- -B
flag <- 1
#g_sub is an interval
}
else{
g_sub_right <- B
g_sub_left  <- 0
flag <- 1
#g_sub is an interval
}
# When flag ==0 ,subgrad is grad, otherwise is a interval
if (flag == 0){
return (abs(alpha[i]-g_sub))
}
#  subgrad is a interval
if (alpha[i] >= g_sub_left & alpha[i] <= g_sub_right){
return (0)
}
else {
return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
# Return the closet distance from the interval
}
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
p.ada.gap <- function(alpha,lambda,B,A,y){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- lasso.gap(alpha, lambda, B, A, i, y)
#browser()
}
#browser()
psum <- sum(p)
for (i in 1:n){
p[i] <- p[i]/psum
}
return(p)
}
p.ada.uniform <- function(alpha, lambda, A, y, sigma,B){
n <- length(alpha)
p <- numeric(length = n)
second_term <- numeric(length = n)
eps <- 1e-5
m <- n
for (i in 1:n){
k <- abs(lasso.dualres(alpha, lambda, A, i,y, B))
if( k < eps){
p[i] <- 0
m <- m-1
}
second_term[i] <- k*norm(as.matrix(A[,i]),type ="F")
#browser()
}
second_term <- second_term/sum(second_term)
for(i in 1:n){
if(second_term[i]<eps){
second_term[i] <- 0
}
}
second_term <- second_term/sum(second_term)
p <- sigma/m + second_term*(1-sigma)
for(i in 1:n){
if(second_term[i] == 0){
p[i] <- 0
}
}
#browser()
return(p)
}
p.uniform <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- 1/n
}
return(p)
}
p.imp <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- norm(as.matrix(A[,i]),type = "F")
}
p = p/sum(p)
return(p)
}
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
#browser()
B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha[i] <- alpha[i] - step0*update/inverses
#alpha_ <- alpha - step0*update/inverses
#if (lasso.loss(A,alpha_,y) < lasso.loss(A,alpha,y) ){
#   alpha <- alpha_
#   iter <- iter+1
#}
#if (iter %%10 == 0){
#  print("w")
#  w <- lasso.w_func(alpha,A,y)
#  print(sum(w**2))}
#browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
#browser()
}
}
result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
#browser()
return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
CD_per_epoch <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
#browser()
B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = p)
update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha[i] <- alpha[i] - step0*update/inverses
#browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
#browser()
}
}
result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
#browser()
return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
A.mushrooms <- matrix(0,nrow = 8124, ncol = 112)
y.mushrooms <- numeric(length = 112)
a <- strsplit(mushrooms, split = " ")
for(i in 1:8124){
b <- a[[i]]
y.mushrooms[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
for(j in 1:len){
colind <- c[[j]][1]
val <- c[[j]][2]
colind <- as.numeric(colind)
val <- as.numeric(val)
A.mushrooms[i,colind] <- val
}
}
set.seed(100)
ind <- sample(1:20242, 8000)
feature <- sample(1:50000, 300)
feature <- as.character(sort(feature))
rcv1 <- rcv1[ind]
A.rcv1 <- matrix(0,nrow = 8000, ncol = 300)
colnames(A.rcv1) <- feature
y.rcv1 <- numeric(length = 8000)
a <- strsplit(rcv1, split = " ")
for(i in 1:8000){
b <- a[[i]]
y.rcv1[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
#browser()
for(j in 1:len){
colind <- c[[j]][1]
if(sum(colind==feature)==1){
val <- c[[j]][2]
val <- as.numeric(val)
A.rcv1[i,colind] <- val
}
}
}
dim(A.mushrooms)
A.mushrooms
y.mushrooms
dim(c)
c
b
y[[1]]
y.mushrooms[[1]]
a
a[[1]]
rrcv1
rcv1
