#inverses <- max(2*sum(A[,i]**2),1)
inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
browser()
}
}
#browser()
return (log(record.gap))
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5,sigma=1)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5,sigma=1)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5)
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.loss <- function(A,alpha,y) {
# 返回loss函数的值
return (norm(as.matrix(A%*%alpha - y),type = "2"))
#+norm("alpha",type = "O"))
}
lasso.w_func <- function(alpha,A,y){
#返回w的函数值
return(2*(A %*% alpha - y))
}
lasso.subgrad <- function(alpha,lambda,A,dimension,y,...){
#返回次梯度
i <- dimension
n <- length(alpha)
#alpha0 <- as.matrix(alpha)
# First calculate the subgrad of |alpha|, sub1
if (alpha[i] > 0 ){
sub1 <- lambda#alpha[i]/norm(alpha0,type = "O")
}
else if(alpha[i] < 0){
sub1 <- -lambda#alpha[i]/norm(alpha0,type = "O")
}
else {
seed <- 0#runif(1,-1,1)
sub1 <- lambda*seed #* alpha[i]/norm(alpha0,type = "O")
}
# Then calculate the grad of ||A\alpha -y||^2
term <- vector(length =length(A[,1]))
for (s in 1:n){
term <- term + as.vector(alpha[s]*A[,s])
}
sub2 <- t(A[,i]) %*% term + t(term) %*% A[,i] - t(y) %*% A[,i] - t(A[,i])%*% y
subgrad = sub1 +rowSums(sub2)
#browser()
return(subgrad)
}
lasso.gap <- function(alpha,lambda,B,A,dimension,y){
# 计算对偶gap
# A is a matrix
i <- dimension
#browser()
w <- lasso.w_func(alpha,A,y)
gap <- B*positive(abs(sum(A[,i]* w))- lambda)+lambda*abs(alpha[i])+alpha[i]*(sum(A[,i]* w))
#browser()
return(gap/exp(15))
}
#CD_each_iter(sample_p = p.uniform,A =diag(1,3,3) ,y = diag(1,3,3)%*%c(1,0,1),lambda = 0.05)
lasso.dualres <- function(alpha,lambda,A,dimension,y, B){
#计算对偶残差
# first calculate the subgrad of g_i^*
i <- dimension
flag <-  0
eps <- 1e-5
w <- lasso.w_func(alpha, A, y)
input <- -t(A[,i])%*%w
#browser()
# 这里的g用的是文中修改后的g拔，下面计算次梯度
if (input <= -lambda - eps){
g_sub <- -B
}
else if(input >= lambda + eps){
g_sub <- B
}
else if(input > -lambda + eps & input < lambda + eps){
g_sub <- 0
}
else if(input < 0){
g_sub_right <- 0
g_sub_left  <- -B
flag <- 1
#g_sub is an interval
}
else{
g_sub_right <- B
g_sub_left  <- 0
flag <- 1
#g_sub is an interval
}
# When flag ==0 ,subgrad is grad, otherwise is a interval
if (flag == 0){
return (abs(alpha[i]-g_sub))
}
#  subgrad is a interval
if (alpha[i] >= g_sub_left & alpha[i] <= g_sub_right){
return (0)
}
else {
return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
# Return the closet distance from the interval
}
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.imp,A = A,y = y,lambda = 0.05,step0 = 0.5)
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
# browser()
B <- (norm(A%*%alpha - y,type = "2"))/lambda + lambda*sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
#inverses <- max(2*sum(A[,i]**2),1)
inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
#browser()
}
}
#browser()
return (log(record.gap))
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.imp,A = A,y = y,lambda = 0.05,step0 = 0.5)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.gap,A = A,y = y,lambda = 0.05,step0 = 0.5)
p.ada.gap <- function(alpha,lambda,B,A,y){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- lasso.gap(alpha, lambda, B, A, i, y)
#browser()
}
#browser()
psum <- sum(p)
for (i in 1:n){
p[i] <- p[i]/psum
}
return(p)
}
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.gap,A = A,y = y,lambda = 0.05,step0 = 0.5)
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
# browser()
B <- (norm(A%*%alpha - y,type = "2"))/lambda + lambda*sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
#inverses <- max(2*sum(A[,i]**2),1)
inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
browser()
}
}
#browser()
return (log(record.gap))
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
CD_each_iter(sample_p = p.ada.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1, sigma=0.5)
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
# browser()
B <- (norm(A%*%alpha - y,type = "2"))/lambda + lambda*sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
#browser()
}
}
#browser()
return (log(record.gap))
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.gap,A = A,y = y,lambda = 0.05,step0 = 0.5, sigma=0.5)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5, sigma=0.5)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.ada.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5, sigma=1)
CD_each_iter(sample_p = p.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5, sigma=1)
CD_each_iter(sample_p = p.uniform,A = A,y = y,lambda = 0.05,step0 = 0.5)
CD_each_iter(sample_p = p.imp,A = A,y = y,lambda = 0.05,step0 = 0.5)
knitr::opts_chunk$set(echo = TRUE)
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
# browser()
B <- (norm(A%*%alpha - y,type = "2"))/lambda + lambda*sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
#browser()
}
}
#browser()
return (log(record.gap))
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
{
record_length <- 25
epoch <- dim(A)[2]
n <- dim(A)[2]
alpha <- numeric(n)
max_iter <- epoch*record_length
record.gap <- numeric(length=record_length)
record.loss <- numeric(length=record_length)
gap <- numeric(n)
# A,y are given data
# browser()
B <- (norm(A%*%alpha - y,type = "2"))/lambda + lambda*sum(abs(alpha))
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
inverses <- max(2*sum(A[,i]**2),1)
#inverses <- 2*sum(A[,i]**2)
alpha <- alpha - step0*update/inverses
# browser()
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
}
record.gap[k] <- sum(gap)
#print(lasso.loss(A = A,alpha = alpha,y = y))
record.loss[k] <- lasso.loss(A, alpha, y)
#browser()
}
}
result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
#browser()
return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.loss <- function(A,alpha,y) {
# 返回loss函数的值
return (norm(as.matrix(A%*%alpha - y),type = "2"))
#+norm("alpha",type = "O"))
}
lasso.w_func <- function(alpha,A,y){
#返回w的函数值
return(2*(A %*% alpha - y))
}
lasso.subgrad <- function(alpha,lambda,A,dimension,y,...){
#返回次梯度
i <- dimension
n <- length(alpha)
#alpha0 <- as.matrix(alpha)
# First calculate the subgrad of |alpha|, sub1
if (alpha[i] > 0 ){
sub1 <- lambda#alpha[i]/norm(alpha0,type = "O")
}
else if(alpha[i] < 0){
sub1 <- -lambda#alpha[i]/norm(alpha0,type = "O")
}
else {
seed <- 0#runif(1,-1,1)
sub1 <- lambda*seed #* alpha[i]/norm(alpha0,type = "O")
}
# Then calculate the grad of ||A\alpha -y||^2
term <- vector(length =length(A[,1]))
for (s in 1:n){
term <- term + as.vector(alpha[s]*A[,s])
}
sub2 <- t(A[,i]) %*% term + t(term) %*% A[,i] - t(y) %*% A[,i] - t(A[,i])%*% y
subgrad = sub1 +rowSums(sub2)
#browser()
return(subgrad)
}
lasso.gap <- function(alpha,lambda,B,A,dimension,y){
# 计算对偶gap
# A is a matrix
i <- dimension
#browser()
w <- lasso.w_func(alpha,A,y)
gap <- B*positive(abs(sum(A[,i]* w))- lambda)+lambda*abs(alpha[i])+alpha[i]*(sum(A[,i]* w))
#browser()
return(gap)
}
#CD_each_iter(sample_p = p.uniform,A =diag(1,3,3) ,y = diag(1,3,3)%*%c(1,0,1),lambda = 0.05)
lasso.dualres <- function(alpha,lambda,A,dimension,y, B){
#计算对偶残差
# first calculate the subgrad of g_i^*
i <- dimension
flag <-  0
eps <- 1e-5
w <- lasso.w_func(alpha, A, y)
input <- -t(A[,i])%*%w
#browser()
# 这里的g用的是文中修改后的g拔，下面计算次梯度
if (input <= -lambda - eps){
g_sub <- -B
}
else if(input >= lambda + eps){
g_sub <- B
}
else if(input > -lambda + eps & input < lambda + eps){
g_sub <- 0
}
else if(input < 0){
g_sub_right <- 0
g_sub_left  <- -B
flag <- 1
#g_sub is an interval
}
else{
g_sub_right <- B
g_sub_left  <- 0
flag <- 1
#g_sub is an interval
}
# When flag ==0 ,subgrad is grad, otherwise is a interval
if (flag == 0){
return (abs(alpha[i]-g_sub))
}
#  subgrad is a interval
if (alpha[i] >= g_sub_left & alpha[i] <= g_sub_right){
return (0)
}
else {
return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
# Return the closet distance from the interval
}
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
CD_each_iter(sample_p = p.imp,A = A,y = y,lambda = 0.05,step0 = 0.5)
uniform <- data.frame(dual.gap, suboptimality)
uniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
uniform$dual.gap <- 1:5
uniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
uniform$dual.gap <- 1:5
uniform
uniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
uniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
ssuniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
adative <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
ada_uniform <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
importance <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
ada_gap <- data.frame(dual.gap = rep(0,5), suboptimality = rep(0,5))
View(importance)
rm("ada_gap","ada_uniform","importance","uniform","ssuniform")
