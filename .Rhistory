knitr::opts_chunk$set(echo = TRUE)
CD <- function(alpha_0,max_iter,sample,loss,loss.grad,w_func,x,y)
{
# x,y are given data
alpha <- alpha_0
w <- w_func(alpha)
iter <- 0
while(iter < max_iter){
direction <- sample(alpha,w,x,y)
# direction is a vector like alpha with only one none zero dim
update <- direction * loss.grad(alpha,x,y,...)
alpha <- alpha + update
w <- w_func(alpha)
}
}
###example
CD()
CD <- function(alpha_0 =0,max_iter,sample,loss,loss.grad,w_func,x,y)
{
# x,y are given data
alpha <- alpha_0
w <- w_func(alpha)
iter <- 0
while(iter < max_iter){
direction <- sample(alpha,w,x,y)
# direction is a vector like alpha with only one none zero dim
update <- direction * loss.grad(alpha,x,y,...)
alpha <- alpha + update
w <- w_func(alpha)
}
}
?norm
svm.loss <- function(alpha,lambda){
n <- length(alpha)
l1 <- 1/n
l2 <- 1*lambda/2
}
svm.gap <- function(alpha,lambda,B,w,A,demension = 1){
# A is a matrix
i <- demension
return(g_conj(-t(A[,i]%*% w)+ g(alpha[i])+alpha[i]*t(A[,i])%*%w)
}
svm.loss <- function(alpha,lambda){
n <- length(alpha)
l1 <- 1/n
l2 <- 1*lambda/2
}
svm.gap <- function(alpha,lambda,B,w,A,y,demension = 1){
# A is a matrix
i <- demension
n <- length(alpha)
g_conj <- function(input,y){
return(input*y/n)
}
g <- function(input,y){
# Not certain
# alpha_i * y_i in [0,1]
return(input * y)
}
return(g_conj(-t(A[,i],y[i])%*% w)+ g(alpha[i],y[i])+alpha[i]* t(A[,i]) %*%w)
}
knitr::opts_chunk$set(echo = TRUE)
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.subgrad <- function(alpha,lambda,demension = 1){
i <- demension
n <- length(alpha)
# First calculate the subgrad of |alpha|
if (alpha)
return(subgrad)
}
lasso.gap <- function(alpha,lambda,B,w,A,demension = 1){
# A is a matrix
i <- demension
return(B*positive(t(A[,i])%*% w- lambda)+lambda*abs(alpha[i])+
alpha[i]*t(A[,i])%*% w)
}
lasso.dualres <- function(alpha,lambda,w,A,demension = i){
# min
}
a = vector(3)
a = vector(length = 3)
a[2]
a[2]*2
a[2] <- 3
sum(a,a)
inf
1/0
?norm
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.w_func(alpha,lambda,A,...){
knitr::opts_chunk$set(echo = TRUE)
mushrooms <- readLines("https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/mushrooms")
rcv1 <- readLines("rcv1_train.binary")
A.mushrooms <- matrix(0,nrow = 8124, ncol = 112)
y.mushrooms <- numeric(length = 112)
a <- strsplit(mushrooms, split = " ")
for(i in 1:8124){
b <- a[[i]]
y.mushrooms[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
for(j in 1:len){
colind <- c[[j]][1]
val <- c[[j]][2]
colind <- as.numeric(colind)
val <- as.numeric(val)
A.mushrooms[i,colind] <- val
}
}
set.seed(1)
ind <- sample(1:20242, 10000)
feature <- sample(1:50000, 1000)
feature <- as.character(sort(feature))
rcv1 <- rcv1[ind]
A.rcv1 <- matrix(0,nrow = 10000, ncol = 1000)
colnames(A.rcv1) <- feature
y.rcv1 <- numeric(length = 10000)
a <- strsplit(rcv1, split = " ")
for(i in 1:10000){
b <- a[[i]]
y.rcv1[i] <- as.numeric(b[1])
b <- b[-1]
len <- length(b)
c <- strsplit(b,split = ":")
for(j in 1:len){
colind <- c[[j]][1]
if(sum(colind==feature)==1){
val <- c[[j]][2]
val <- as.numeric(val)
A.rcv1[i,colind] <- val
}
}
}
rs<-apply(A.rcv1,1,sum)
A.rcv1 <- A.rcv1[which(rs!=0),]
y.rcv1 <- y.rcv1[which(rs!=0)]
cs <- apply(a.rcv1,2,sum)
rs<-apply(A.rcv1,1,sum)
A.rcv1 <- A.rcv1[which(rs!=0),]
y.rcv1 <- y.rcv1[which(rs!=0)]
cs <- apply(A.rcv1,2,sum)
A.rcv1 <- A.rcv1[,which(cs!=0)]
dim(A.rcv1)
CD_each_iter(p.ada.gap,A.mushrooms,y.mushrooms,0.05)
positive <- function(x){
if(x >= 0){
return(x)
}
return(0)
}
lasso.loss <- function(A,alpha,y) {
# 返回loss函数的值
return (norm(A%*%alpha - y,type = "2")+norm("alpha",type = "O"))
}
lasso.w_func <- function(alpha,A,y){
#返回w的函数值
return(2*t(A)%*%(A %*% alpha - y))
}
lasso.subgrad <- function(alpha,lambda,A,dimension){
#返回次梯度
i <- dimension
n <- length(alpha)
# First calculate the subgrad of |alpha|, sub1
if (alpha[i] > 0 ){
sub1 <- alpha[i]/norm(alpha,type = "O")
}
else if(alpha[i] < 0){
sub1 <- alpha[i]/norm(alpha,type = "O")
}
else {
seed <- runif(1,-1,1)
sub1 <- lambda*seed * alpha[i]/norm(alpha,type = "O")
}
# Then calculate the grad of ||A\alpha -y||^2
term <- vector(length =n)
for (s in 1:n){
term <- term + alpha[s]*A[s]
}
sub2 <- t(A[,i]) %*% term + t(term) %*% A[,i]
- t(y) %*% A[,i] - t(A[,i])%*% y
subgrad = sub1 +sub2
return(subgrad)
}
lasso.gap <- function(alpha,lambda,B,A,dimension,y){
# 计算对偶gap
# A is a matrix
i <- dimension
w <- lasso.w_func(alpha,A,y)
return(B*positive(t(A[,i])%*% w- lambda)+lambda*abs(alpha[i])+
alpha[i]*t(A[,i])%*% w)
}
lasso.dualres <- function(alpha,lambda,A,dimension,y){
#计算对偶残差
# first calculate the subgrad of g_i^*
i <- dimension
flag <-  0
eps <- 1e-5
w <- lasso.w_func(alpha, A, y)
input <- -t(A[,i])%*%w
# 这里的g用的是文中修改后的g拔，下面计算次梯度
if (input > eps && input <= B){
g_sub <- lambda *input
}
else if(input < -eps && input >= -B){
g_sub <- -lambda *input
}
else if(abs(input) <B){
g_sub_right <- lambda * abs(input)
g_sub_left  <- -lambda * abs(input)
flag <- 1
#g_sub is an interval
}
else {
g_sub <- Inf
}
# When flag ==0 ,subgrad is grad, otherwise is a interval
if (flag == 0){
return (abs(alpha[i])-g_sub)
}
#  subgrad is a interval
if (abs(alpha[i]) <= g_sub_right){
return (0)
}
else {
return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
# Return the closet distance from the interval
}
}
p.ada.gap <- function(alpha,lambda,B,A,y){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- lasso.gap(alpha, lambda, B, A, i, y)
}
psum <- sum(p)
p[i] <- p[i]/psum
return(p)
}
p.ada.uniform <- function(alpha, lamda, A, y, sigma,B){
n <- length(alpha)
p <- numeric(length = n)
second_term <- numeric(length = n)
eps <- 1e-5
m <- n
for (i in 1:n){
k <- abs(lasso.dualres(alpha, lambda, A, i,y))
if( k < eps){
p[i] <- 0
m <- m-1
}
second_term[i] <- k*norm(A[,i],type ="F")
}
second_term <- second_term/sum(second_term)
for (i in 1:n){
if (p[i] > 0){
p[i] <- sigma/m + second_term*(1-sigma)
}
}
return(p)
}
p.uniform <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- 1/n
}
return(p)
}
p.imp <- function(alpha,A,y,lambda,B){
n <- length(alpha)
p <- numeric(length = n)
for (i in 1:n){
p[i] <- norm(A[,i],type = "F")
}
p = p/sum(p)
return(p)
}
CD_each_iter <- function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[2]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- lasso.w_func(alpha, A,y)/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
}
}
return (log(record.gap))
}
CD_per_epoch <-function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[1]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- lasso.w_func(alpha, A,y)/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = p)
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
}
}
return (log(record.gap))
}
CD_each_iter(p.ada.gap,A.mushrooms,y.mushrooms,0.05)
View(CD_each_iter)
CD_each_iter <- function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[2]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- (norm(A%*%alpha - y,type = "2"))/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
}
}
return (log(record.gap))
}
CD_per_epoch <-function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[1]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- lasso.w_func(alpha, A,y)/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = p)
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
}
}
return (log(record.gap))
}
CD_each_iter <- function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[2]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- (norm(A%*%alpha - y,type = "2"))/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
}
}
return (log(record.gap))
}
CD_per_epoch <-function(sample_p,A,y,lambda, ...)
{
epoch <- dim(A)[1]
n <- dim(A)[2]
max_iter <- epoch*25
record.gap <- numeric(length=25)
gap <- numeric(n)
# A,y are given data
alpha <- numeric(length = n)
B <- (norm(A%*%alpha - y,type = "2"))/lambda
w <- lasso.w_func(alpha,A,y)
iter <- 0
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
while(iter < max_iter){
iter <- iter+1
i <- sample(1:n,prob = p)
# direction is a sample prob vector like alpha with only one none zero dim
direction = vector(length = n)
direction[i] = 1
update <- direction * lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
alpha[i] <- alpha[i] - update
# Record the result
if(iter%%epoch==0){
k <- iter/epoch
for(j in 1:n){
gap[j] <- lasso.gap(alpha, lambda, B, A, j)
}
record.gap[k] <- sum(gap)
p <- sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...)
}
}
return (log(record.gap))
}
CD_each_iter(p.ada.gap,A.mushrooms,y.mushrooms,0.05)
