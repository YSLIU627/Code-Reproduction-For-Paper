---
title: "homework"
author: "LZH-XX"
date: "12/5/2020"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


##1. The optimization target and their dual gap and dual residual

### Lasso

$\bar{g}_{i}^{*}\left(u_{i}\right)=\max _{\alpha_{i}:\left|\alpha_{i}\right| \leq B} u_{i} \alpha_{i}-\lambda\left|\alpha_{i}\right|=B\left[\left|u_{i}\right|-\lambda\right]_{+}$

```{r positive}
positive <- function(x){
  if(x >= 0){
    return(x)
  }
  return(0)
}
```

```{r lasso.loss}
lasso.loss <- function(A,alpha,y) {
  # 返回loss函数的值
  return (norm(as.matrix(A%*%alpha - y),type = "2"))
          #+norm("alpha",type = "O"))
}
```

```{r lasso.w_func}
lasso.w_func <- function(alpha,A,y){
  #返回w的函数值
  return(2*(A %*% alpha - y))
}
```

```{r lasso.subgrad}
lasso.subgrad <- function(alpha,lambda,A,dimension,y,...){
  #返回次梯度
  i <- dimension
  n <- length(alpha)
  #alpha0 <- as.matrix(alpha)
  # First calculate the subgrad of |alpha|, sub1
  if (alpha[i] > 0 ){
    sub1 <- lambda#alpha[i]/norm(alpha0,type = "O")
  }
  else if(alpha[i] < 0){
    sub1 <- -lambda#alpha[i]/norm(alpha0,type = "O")
  }
  else {
    seed <- 0#runif(1,-1,1)
    sub1 <- lambda*seed #* alpha[i]/norm(alpha0,type = "O")
  }
  # Then calculate the grad of ||A\alpha -y||^2
  #term <- vector(length =length(A[,1]))
  #for (s in 1:n){
  #  term <- term + as.vector(alpha[s]*A[,s])
  #}
  #sub2 <- t(A[,i]) %*% term + t(term) %*% A[,i] - t(y) %*% A[,i] - t(A[,i])%*% y
  term <- 2*(A%*%alpha-y)
  term <- as.vector(term)
  sub2 <- sum(A[,i]*term)
  subgrad = sub1 + sub2 #rowSums(sub2)
  #browser()
  return(subgrad)
}
```

```{r lasso.gap}
lasso.gap <- function(alpha,lambda,B,A,dimension,y){
  # 计算对偶gap
  # A is a matrix
  i <- dimension
  #browser()
  w <- lasso.w_func(alpha,A,y)
  gap <- B*positive(abs(sum(A[,i]* w))- lambda)+lambda*abs(alpha[i])+alpha[i]*(sum(A[,i]* w))
  #browser()
  return(gap)
}
#CD_each_iter(sample_p = p.uniform,A =diag(1,3,3) ,y = diag(1,3,3)%*%c(1,0,1),lambda = 0.05)
```

```{r lasso.dualres}
lasso.dualres <- function(alpha,lambda,A,dimension,y, B){
  #计算对偶残差
  # first calculate the subgrad of g_i^*
  i <- dimension
  flag <-  0
  eps <- 1e-5
  w <- lasso.w_func(alpha, A, y)
  input <- -t(A[,i])%*%w
  #browser()
  # 这里的g用的是文中修改后的g拔，下面计算次梯度
  if (input <= -lambda - eps){
    g_sub <- -B
  }
  else if(input >= lambda + eps){
    g_sub <- B
  }
  else if(input > -lambda + eps & input < lambda + eps){
    g_sub <- 0
  }
  else if(input < 0){
    g_sub_right <- 0
    g_sub_left  <- -B
    flag <- 1 
    #g_sub is an interval
  }
  else{
    g_sub_right <- B
    g_sub_left  <- 0
    flag <- 1 
    #g_sub is an interval
  }
  # When flag ==0 ,subgrad is grad, otherwise is a interval
  if (flag == 0){
      return (abs(alpha[i]-g_sub))
  }
  #  subgrad is a interval
  if (alpha[i] >= g_sub_left & alpha[i] <= g_sub_right){
    return (0)
  }
  else {
    return (min(abs(alpha[i]-g_sub_right),abs(alpha[i]-g_sub_left) ))
    # Return the closet distance from the interval
  }
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
```
##2.Adaptive Sampling -based CD 

###2.1 Gap-wise
```{r p.ada.gap}
p.ada.gap <- function(alpha,lambda,B,A,y){
  n <- length(alpha)
  p <- numeric(length = n)
  for (i in 1:n){
    p[i] <- lasso.gap(alpha, lambda, B, A, i, y)
    #browser()
  }
  #browser()
  psum <- sum(p) 
  for (i in 1:n){
    p[i] <- p[i]/psum
  }
  return(p)
}
```

###2.2 Adaptive

```{r p.ada.uniform}
p.ada.uniform <- function(alpha, lambda, A, y, sigma,B){
  n <- length(alpha)
  p <- numeric(length = n)
  second_term <- numeric(length = n)
  eps <- 1e-5
  m <- n
  for (i in 1:n){
    k <- abs(lasso.dualres(alpha, lambda, A, i,y, B))
    if( k < eps){
      p[i] <- 0
      m <- m-1
    }
    second_term[i] <- k*norm(as.matrix(A[,i]),type ="F")
    #browser()
  }
  second_term <- second_term/sum(second_term)
  for(i in 1:n){
    if(second_term[i]<eps){
      second_term[i] <- 0
    }
  }
  second_term <- second_term/sum(second_term)
  p <- sigma/m + second_term*(1-sigma)
  for(i in 1:n){
    if(second_term[i] == 0){
      p[i] <- 0
    }
  }
  #browser()
  return(p)
}
```


###2.3 Uniform

```{r p.uniform}
p.uniform <- function(alpha,A,y,lambda,B){
  n <- length(alpha)
  p <- numeric(length = n)
  for (i in 1:n){
    p[i] <- 1/n
  }
  return(p)
}
```

###2.4 Importance Sampling

```{r p.imp}
p.imp <- function(alpha,A,y,lambda,B){
  n <- length(alpha)
  p <- numeric(length = n)
  for (i in 1:n){
    p[i] <- norm(as.matrix(A[,i]),type = "F")
  }
  p = p/sum(p)
  return(p)
}
```


##3.Coordinate descdent

```{r CD_each_iter}
CD_each_iter <- function(sample_p,A,y,lambda,step0 = 0.1,...)
  {
  record_length <- 25
  epoch <- dim(A)[2]
  n <- dim(A)[2]
  alpha <- numeric(n)
  max_iter <- epoch*record_length
  record.gap <- numeric(length=record_length)
  record.loss <- numeric(length=record_length)
  gap <- numeric(n)
  # A,y are given data
  #browser()
  B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
  w <- lasso.w_func(alpha,A,y)
  iter <- 0
  while(iter < max_iter){
  iter <- iter+1
  i <- sample(1:n,size = 1,prob = sample_p(alpha = alpha,A=A,y=y,lambda=lambda,B=B,...))
  
  update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
  inverses <- max(2*sum(A[,i]**2),1)
  #inverses <- 2*sum(A[,i]**2)
  alpha[i] <- alpha[i] - step0*update/inverses
  #alpha_ <- alpha - step0*update/inverses
  #if (lasso.loss(A,alpha_,y) < lasso.loss(A,alpha,y) ){
  #   alpha <- alpha_ 
  #   iter <- iter+1
  #}
  #if (iter %%10 == 0){
  #  print("w")
  #  w <- lasso.w_func(alpha,A,y)
  #  print(sum(w**2))}
  #browser()
  # Record the result
  if(iter%%epoch==0){
    k <- iter/epoch
    for(j in 1:n){
      gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
    }
    record.gap[k] <- sum(gap)
    #print(lasso.loss(A = A,alpha = alpha,y = y))
    record.loss[k] <- lasso.loss(A, alpha, y)
    #browser()
  }
  }
  result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
  #browser()
  return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
```

```{r CD_per_epoch}
CD_per_epoch <- function(sample_p,A,y,lambda,step0 = 0.1,...)
  {
  record_length <- 25
  epoch <- dim(A)[2]
  n <- dim(A)[2]
  alpha <- numeric(n)
  max_iter <- epoch*record_length
  record.gap <- numeric(length=record_length)
  record.loss <- numeric(length=record_length)
  gap <- numeric(n)
  # A,y are given data
  #browser()
  B <- (norm(A%*%alpha - y,type = "2"))^2/lambda + sum(abs(alpha))
  w <- lasso.w_func(alpha,A,y)
  iter <- 0
  p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
  while(iter < max_iter){
  iter <- iter+1
  i <- sample(1:n,size = 1,prob = p)
  update <- lasso.subgrad(alpha = alpha,dimension =i ,A =A,y=y,lambda = lambda)
  inverses <- max(2*sum(A[,i]**2),1)
  #inverses <- 2*sum(A[,i]**2)
  alpha[i] <- alpha[i] - step0*update/inverses
  #browser()
  # Record the result
  if(iter%%epoch==0){
    k <- iter/epoch
    for(j in 1:n){
      gap[j] <- lasso.gap(alpha, lambda, B, A, j,y=y)
    }
    record.gap[k] <- sum(gap)
    #print(lasso.loss(A = A,alpha = alpha,y = y))
    record.loss[k] <- lasso.loss(A, alpha, y)
    p <- sample_p(alpha = alpha, A = A, y = y, lambda = lambda, B = B, ...)
    #browser()
  }
  }
  result <- list(dual.gap = log(record.gap), suboptimality = log(record.loss))
  #browser()
  return (result)
}
#CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05)
```


##4.Exmperiment

Collect data
```{r get_dataset, eval=FALSE}
mushrooms <- readLines("https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/binary/mushrooms")
rcv1 <- readLines("rcv1_train.binary")
```
Get corresponding A and y
```{r mushrooms_A&y, eval=FALSE}
A.mushrooms <- matrix(0,nrow = 8124, ncol = 112)
y.mushrooms <- numeric(length = 112)
a <- strsplit(mushrooms, split = " ")
for(i in 1:8124){
  b <- a[[i]]
  y.mushrooms[i] <- as.numeric(b[1])
  b <- b[-1]
  len <- length(b)
  c <- strsplit(b,split = ":")
  for(j in 1:len){
    colind <- c[[j]][1]
    val <- c[[j]][2]
    colind <- as.numeric(colind)
    val <- as.numeric(val)
    A.mushrooms[i,colind] <- val
  }
}
```

```{r rcv1_A&y, eval=FALSE}
set.seed(100)
ind <- sample(1:20242, 8000)
feature <- sample(1:50000, 300)
feature <- as.character(sort(feature))
rcv1 <- rcv1[ind]
A.rcv1 <- matrix(0,nrow = 8000, ncol = 300)
colnames(A.rcv1) <- feature
y.rcv1 <- numeric(length = 8000)
a <- strsplit(rcv1, split = " ")
for(i in 1:8000){
  b <- a[[i]]
  y.rcv1[i] <- as.numeric(b[1])
  b <- b[-1]
  len <- length(b)
  c <- strsplit(b,split = ":")
  #browser()
  for(j in 1:len){
    colind <- c[[j]][1]
    if(sum(colind==feature)==1){
      val <- c[[j]][2]
      val <- as.numeric(val)
      A.rcv1[i,colind] <- val
    }
  }
}
```
remove rows and columns that are full of 0 in A.rcv1
```{r pre_process_rcv1, eval=FALSE}
rs<-apply(A.rcv1,1,sum)
A.rcv1 <- A.rcv1[which(rs!=0),]
y.rcv1 <- y.rcv1[which(rs!=0)]
cs <- apply(A.rcv1,2,sum)
A.rcv1 <- A.rcv1[,which(cs!=0)]
```
example

```{r glm_try, eval=FALSE}
glmnet.fit<- cv.glmnet( A.mushrooms,y.mushrooms ,family = "multinomial")
g.coef <- coef(glmnet.fit)
n=112
alpha1 = numeric(n)
for (mmm in 1:n){
  alpha1[mmm] = runif(1,-1,-0.5)#g.coef[[1]][mmm]
}
sum((A.mushrooms%*%alpha1-y.mushrooms)^2)
n=112
alpha1 = numeric(n)
for (mmm in 1:n){
  alpha1[mmm] = g.coef[[1]][mmm]
}
```


```{r small_data_for_debug, eval=FALSE}
d = length(alpha1)
A = A.mushrooms[1:200,]
y = A%*%alpha1
alpha = numeric(d)
#CD_each_iter(sample_p = p.imp,A = A,y = y,lambda = 0.05,step0 = 0.5)
```

```{r full_data_for_degug, eval=FALSE}
CD_each_iter(sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
```

## 5.Plot


```{r save_result, eval=TRUE}
save_result <- function(CD, name, ...){
  result.gap <- matrix(0,nrow = 5, ncol = 25)
  result.loss <- matrix(0, nrow = 5, ncol = 25)
  for (i in 1:5){
    result <- CD(...)
   result.gap[i,] <- result$dual.gap
   result.loss[i,] <- result$suboptimality
  }
  name1 <- paste0("result/", name, "_gap.csv")
  name2 <- paste0("result/", name, "_loss.csv")
  write.csv(as.data.frame(result.gap),file = name1)
  write.csv(as.data.frame(result.loss),file = name2)
}
```

```{r collect_data_each_iter_mushrooms, eval=FALSE}
save_result(CD_each_iter, "uni", sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
save_result(CD_each_iter, "ssuni", sample_p = p.ada.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1,sigma=1)
save_result(CD_each_iter, "adative", sample_p = p.ada.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1,sigma=0)
save_result(CD_each_iter, "adauni", sample_p = p.ada.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1,sigma=0.5)
save_result(CD_each_iter, "imp", sample_p = p.imp,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
save_result(CD_each_iter, "adagap", sample_p = p.ada.gap,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
```


```{r collect_data_per_epoch_mushrooms,  eval=FALSE}
save_result(CD_per_epoch, "epoch_adagap", sample_p = p.ada.gap,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
save_result(CD_per_epoch, "epoch_imp", sample_p = p.imp,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
save_result(CD_each_iter, "epoch_uni", sample_p = p.uniform,A = A.mushrooms,y = y.mushrooms,lambda = 0.05,step0 = 1)
```

```{r result_cv1, echo=FALSE}
#save_result(CD_each_iter, "uni_rcv", sample_p = p.uniform,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)
save_result(CD_each_iter, "ssuni_rcv", sample_p = p.ada.uniform,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1, sigma = 1)
save_result(CD_each_iter, "adative_rcv", sample_p = p.ada.uniform,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1, sigma = 0)
save_result(CD_each_iter, "adauni_rcv", sample_p = p.ada.uniform,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1, sigma = 0.5)
save_result(CD_each_iter, "imp_rcv", sample_p = p.imp,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)
save_result(CD_each_iter, "adagap_rcv", sample_p = p.ada.gap,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)


save_result(CD_per_epoch, "epoch_uni_rcv", sample_p = p.uniform,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)
save_result(CD_per_epoch, "epoch_imp_rcv", sample_p = p.imp,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)
save_result(CD_per_epoch, "epoch_adagap_rcv", sample_p = p.ada.gap,A = A.rcv1,y = y.rcv1,lambda = 7e-4,step0 = 1)
```

```{r}
process_result <- function(df){
  return(apply(df, 2, mean))
}
```

```{r}
gap.adagap.e <- process_result(read.csv("result/epoch_adagap_gap.csv"))[-1]
gap.uni.e <- process_result(read.csv("result/epoch_uni_gap.csv"))[-1]
gap.imp.e <- process_result(read.csv("result/epoch_imp_gap.csv"))[-1]
loss.adagap.e <- process_result(read.csv("result/epoch_adagap_loss.csv"))[-1]
loss.uni.e <- process_result(read.csv("result/epoch_uni_loss.csv"))[-1]
loss.imp.e <- process_result(read.csv("result/epoch_imp_loss.csv"))[-1]
plot(1:25, gap.adagap.e, col = "yellow", xlab = "Epochs", ylab = "log of dual gap", type = "l")
par(new = TRUE)
plot(1:25, gap.imp.e, col = "red", xlab = "Epochs", ylab = "log of dual gap", type = "l")
par(new = TRUE)
plot(1:25, gap.uni.e, col = "blue", xlab = "Epochs", ylab = "log of dual gap", type = "l")
par(new = TRUE)
legend("topright", col = c("blue", "red", "yellow"),lty = 1, legend = c("uniform","importance", "gap-per-epoch"))
```